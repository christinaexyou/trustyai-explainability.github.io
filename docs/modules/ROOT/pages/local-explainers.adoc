// Settings
:idprefix:
:idseparator: -
:example-caption!:
:stem: latexmath

= Local explainers

== LIME

Local Interpretable Model-agnostic Explanations (LIME) <<ribeiro2016>> is a saliency ex-
planation method. LIME aims to explain a prediction stem:[p = (x, y)] (an input-
output pair) generated by a black box model stem:[f : \mathbb{R}^d \rightarrow \mathbb{R}]. Such explanations
come in the form of a “saliency” stem:[w_i] attached to each feature stem:[x_i] in the prediction
input stem:[x].
LIME generates a local explanation stem:[\xi(x)] according to the following model:

[stem]
++++
\xi(x) = \arg\min_{g \in G}L(f, g, \pi_x) + \Omega(g)
++++

where stem:[\pi_x] is a proximity function, stem:[G] the family of interpretable models, stem:[\Omega(g)] is a measure of complexity of an explanation stem:[g \in G] and stem:[L(f, g, \pi_x)] is a measure
of how unfaithful stem:[g] is in approximating stem:[f] in the locality defined by stem:[\pi_x]. In the
original paper, G is the class of linear models, πx is an exponential kernel on
a distance function stem:[D] (e.g. cosine distance). LIME converts samples stem:[x_i] from the original domain into intepretable samples as binary vectors stem:[x^{\prime}_i \in {0, 1}]. An
encoded dataset stem:[E] is built by taking non-zero elements of stem:[x^{\prime}_i], recovering the
original representation stem:[z \in \mathbb{R}^d] and then computing stem:[f(z)]. A weighted linear model stem:[g] (with weights provided via stem:[\pi_x]) is then trained upon the generated
sparse dataset stem:[E] and the model weights stem:[w] are used as feature weights for the final explanation stem:[\xi(x)].

[bibliography]
== References

* [[[ribeiro2016]]] Ribeiro, M.T., Singh, S., Guestrin, C.: ” why should i trust you?” explaining the predictions of any classifier. In: Proceedings of the 22nd ACM
SIGKDD international conference on knowledge discovery and data mining,
pp. 1135–1144 (2016)
